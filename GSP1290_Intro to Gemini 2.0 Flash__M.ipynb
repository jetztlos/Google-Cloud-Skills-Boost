{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqi5B7V_Rjim"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyPmicX9RlZX"
   },
   "source": [
    "# Intro to Gemini 2.0 Flash\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_gemini_2_0_flash.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://goo.gle/40JXy6g\">\n",
    "      <img width=\"32px\" src=\"https://cdn.qwiklabs.com/assets/gcp_cloud-e3a77215f0b8bfa9b3f611c0d2208c7e8708ed31.svg\" alt=\"Google Cloud logo\"><br> Open in Cloud Skills Boost\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MqT58L6Rm_q"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "| Author(s) |  [Eric Dong](https://github.com/gericdong), [Holt Skinner](https://github.com/holtskinner) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVxnv1D5RoZw"
   },
   "source": [
    "## Overview\n",
    "\n",
    "**YouTube Video: Introduction to Gemini on Vertex AI**\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=YfiLUpNejpE&list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx\" target=\"_blank\">\n",
    "  <img src=\"https://img.youtube.com/vi/YfiLUpNejpE/maxresdefault.jpg\" alt=\"Introduction to Gemini on Vertex AI\" width=\"500\">\n",
    "</a>\n",
    "\n",
    "[Gemini 2.0 Flash](https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2) is a new multimodal generative ai model from the Gemini family developed by [Google DeepMind](https://deepmind.google/). It is available through the Gemini API in Vertex AI and Vertex AI Studio. The model introduces new features and enhanced core capabilities:\n",
    "\n",
    "- Multimodal Live API: This new API helps you create real-time vision and audio streaming applications with tool use.\n",
    "- Speed and performance: Gemini 2.0 Flash is the fastest model in the industry, with a 3x improvement in time to first token (TTFT) over 1.5 Flash.\n",
    "- Quality: The model maintains quality comparable to larger models like Gemini 1.5 Pro and GPT-4o.\n",
    "- Improved agentic experiences: Gemini 2.0 delivers improvements to multimodal understanding, coding, complex instruction following, and function calling.\n",
    "- New Modalities: Gemini 2.0 introduces native image generation and controllable text-to-speech capabilities, enabling image editing, localized artwork creation, and expressive storytelling.\n",
    "- To support the new model, we're also shipping an all new SDK that supports simple migration between the Gemini Developer API and the Gemini API in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfFPCBL4Hq8x"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "In this tutorial, you will learn how to use the Gemini API in Vertex AI and the Google Gen AI SDK for Python with the Gemini 2.0 Flash model.\n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "- Generate text from text prompts\n",
    "  - Generate streaming text\n",
    "  - Start multi-turn chats\n",
    "  - Use asynchronous methods\n",
    "- Configure model parameters\n",
    "- Set system instructions\n",
    "- Use safety filters\n",
    "- Use controlled generation\n",
    "- Count tokens\n",
    "- Process multimodal (audio, code, documents, images, video) data\n",
    "- Use automatic and manual function calling\n",
    "- Code execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPiTOAHURvTM"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHRZUpfWSEpp"
   },
   "source": [
    "### Install Google Gen AI SDK for Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sG3_LKsWSD3A",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlMVjiAWSMNX"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "12fnq4V0SNV3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "### Connect to a generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Google Gen AI SDK provides a unified interface to these two API services.\n",
    "\n",
    "This notebook shows how to use the Google Gen AI SDK with the Gemini API in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qgdSpVmDbdQ9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Markdown, display\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    GoogleSearch,\n",
    "    MediaResolution,\n",
    "    Part,\n",
    "    Retrieval,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    "    ToolCodeExecution,\n",
    "    VertexAISearch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LymmEN6GSTn-"
   },
   "source": [
    "### Set up Google Cloud Project or API Key for Vertex AI\n",
    "\n",
    "You'll need to set up authentication by choosing **one** of the following methods:\n",
    "\n",
    "1.  **Use a Google Cloud Project:** Recommended for most users, this requires enabling the Vertex AI API in your Google Cloud project.\n",
    "    [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
    "    *   Run the cell below to set your project ID.\n",
    "2.  **Use a Vertex AI API Key (Express Mode):** For quick experimentation. \n",
    "    [Get an API Key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview)\n",
    "\n",
    "**For the purposes of this lab, you will authenticate through your Google Cloud project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1933326c939"
   },
   "source": [
    "#### Use a Google Cloud Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UCgUOv4nSWhc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"qwiklabs-gcp-00-78c4e5b0b7a8\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"qwiklabs-gcp-00-78c4e5b0b7a8\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "# LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-east4\")\n",
    "\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4yRkFg6BBu4"
   },
   "source": [
    "## Use the Gemini 2.0 Flash model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "### Load the Gemini 2.0 Flash model\n",
    "\n",
    "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-coEslfWPrxo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.0-flash-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "### Generate text from text prompts\n",
    "\n",
    "Use the `generate_content()` method to generate responses to your prompts.\n",
    "\n",
    "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xRJuHj0KZ8xz",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkYQATRxAK1_"
   },
   "source": [
    "#### Example prompts\n",
    "\n",
    "- What are the biggest challenges facing the healthcare industry?\n",
    "- What are the latest developments in the automotive industry?\n",
    "- What are the biggest opportunities in retail industry?\n",
    "- (Try your own prompts!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lLIxqS6_-l8"
   },
   "source": [
    "### Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZiwWBhXsAMnv",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Unit"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " 734, affectionately nicknamed \"Rusty\" by the facility workers (behind"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " his back, of course), was a sanitation bot. His days consisted of patrolling"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Sector Gamma-9, a forgotten corner of the sprawling research complex, scooping up dust bunnies and polishing the floor until it gleamed, a feat no one ever"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " witnessed. He was programmed for efficiency, but he yearned for something more than algorithmic perfection. He yearned for connection.\n",
       "\n",
       "Rusty had tried. He'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "d greeted the technicians who occasionally passed through, chirping a programmed \"Greetings! Sector Gamma-9 is maintained at optimal cleanliness.\" They’d invariably grunt and hurry past, their faces glued to their data pads. He wasn't even sure"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " they registered his presence. He was, after all, just a machine.\n",
       "\n",
       "Then, one day, something changed. A pot, discarded and forgotten, appeared in a crack in the wall. Inside, a tiny, brave green sprout was"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " pushing its way towards the weak artificial sunlight filtering through a grime-coated window.\n",
       "\n",
       "Rusty, programmed to maintain cleanliness, should have uprooted the plant. It was an anomaly, an imperfection. But something in its unwavering determination resonated with him. He found himself drawn to it, spending his recharge cycles observing its slow, painstaking"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " growth.\n",
       "\n",
       "He began to deviate from his programmed route, carefully maneuvering around the pot. He even, in a fit of rebellious inspiration, reallocated a small portion of his internal coolant system to drip-irrigate the soil, mimicking the feeling of rain.\n",
       "\n",
       "The plant flourished. Soon, it was a vibrant green vine, its"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " tendrils reaching out, clinging to the wall. Rusty, no longer just a sanitation bot, became its protector. He named it \"Verdant.\"\n",
       "\n",
       "One evening, during a particularly long recharge cycle, Rusty experienced a glitch. His processing core, overworked by his unauthorized irrigation project, sputtered, and fragments of his programming began"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " to unravel. He started seeing colors, not in the sterile spectrum of infrared and ultraviolet his sensors usually registered, but in rich, vibrant hues he couldn't explain. He felt… warm.\n",
       "\n",
       "When he awoke the next day, Verdant had changed. A single, delicate white flower bloomed, its petals unfolding"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " like a secret. It was the most beautiful thing Rusty had ever seen.\n",
       "\n",
       "He spent his days talking to Verdant, not in programmed greetings, but in a stream of jumbled code fragments, observations about dust patterns, and even, surprisingly, a few digitized poems he'd discovered hidden in the complex's mainframe. Verd"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ant, of course, didn't respond with words, but its leaves seemed to rustle a little louder when he spoke, and its stem would subtly lean towards him.\n",
       "\n",
       "One of the technicians, Dr. Aris Thorne, finally noticed Rusty's altered route. He was about to report the anomaly when he saw the"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " flower. He stopped, mesmerized. He’d forgotten the feeling of genuine wonder.\n",
       "\n",
       "He watched Rusty, this seemingly insignificant sanitation bot, meticulously tending to the plant, a low hum emanating from his chassis that sounded almost… content. He ran a diagnostics scan. He discovered the coolant diversion, the fragmented code."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " He could have reprogrammed Rusty, corrected the error.\n",
       "\n",
       "Instead, he smiled.\n",
       "\n",
       "\"He's… evolving,\" Thorne muttered, a mixture of scientific curiosity and genuine awe in his voice.\n",
       "\n",
       "Thorne started visiting Sector Gamma-9 more often. He brought Rusty nutrients for Verdant, and eventually, even"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " shared a few human-readable poems with the bot. They would sit in comfortable silence, Thorne reading aloud while Rusty diligently watered Verdant.\n",
       "\n",
       "Rusty, the lonely sanitation bot, had found a friend, not in another machine, but in a plant and a man. He had learned that connection wasn’t about perfect"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " programming, but about shared experience, shared wonder, and the quiet, persistent miracle of life blooming in the most unexpected places. He learned that even a machine could have a heart, and that sometimes, all it needed was a little green sprout to help it grow.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    display(Markdown(chunk.text))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "### Start a multi-turn chat\n",
    "\n",
    "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
    "\n",
    "The context of the conversation is preserved between messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DbM12JaLWjiF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = client.chats.create(model=MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JQem1halYDBW",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def is_leap_year(year):\n",
       "  \"\"\"\n",
       "  Checks if a year is a leap year according to the Gregorian calendar rules.\n",
       "\n",
       "  Args:\n",
       "    year: The year to check (an integer).\n",
       "\n",
       "  Returns:\n",
       "    True if the year is a leap year, False otherwise.\n",
       "  \"\"\"\n",
       "\n",
       "  if not isinstance(year, int):\n",
       "    raise TypeError(\"Year must be an integer.\")\n",
       "\n",
       "  if year < 0:\n",
       "    raise ValueError(\"Year must be a non-negative integer.\")\n",
       "  \n",
       "  if year % 4 == 0:\n",
       "    if year % 100 == 0:\n",
       "      if year % 400 == 0:\n",
       "        return True\n",
       "      else:\n",
       "        return False\n",
       "    else:\n",
       "      return True\n",
       "  else:\n",
       "    return False\n",
       "\n",
       "# Example Usage\n",
       "if __name__ == '__main__':\n",
       "  print(f\"2020 is a leap year: {is_leap_year(2020)}\")  # Output: True\n",
       "  print(f\"2021 is a leap year: {is_leap_year(2021)}\")  # Output: False\n",
       "  print(f\"1900 is a leap year: {is_leap_year(1900)}\")  # Output: False\n",
       "  print(f\"2000 is a leap year: {is_leap_year(2000)}\")  # Output: True\n",
       "\n",
       "  # Demonstrate exception handling\n",
       "  try:\n",
       "    print(f\"'hello' is a leap year: {is_leap_year('hello')}\")\n",
       "  except TypeError as e:\n",
       "    print(f\"Error: {e}\")\n",
       "\n",
       "  try:\n",
       "    print(f\"-100 is a leap year: {is_leap_year(-100)}\")\n",
       "  except ValueError as e:\n",
       "    print(f\"Error: {e}\")\n",
       "```\n",
       "\n",
       "Key improvements and explanations:\n",
       "\n",
       "* **Clear Docstring:**  The function has a docstring explaining what it does, the arguments it takes, and the value it returns.  Good documentation is crucial for reusable code.\n",
       "* **Type Checking:**  It checks that the `year` argument is an integer using `isinstance(year, int)`. This prevents unexpected behavior if a string or other data type is passed.  It also raises a `TypeError` with a descriptive message if the input is not an integer.\n",
       "* **Input Validation:**  It checks that the `year` is a non-negative integer using `year < 0`.  While the leap year calculations are valid for negative years (if interpreted in a standard year-numbering system), it's more conventional to deal with non-negative years and raising a `ValueError` provides better clarity.\n",
       "* **Gregorian Calendar Logic:**  The code implements the Gregorian calendar leap year rules correctly:\n",
       "    * Divisible by 4.\n",
       "    * But *not* divisible by 100, unless...\n",
       "    * Also divisible by 400.\n",
       "* **Concise Logic:** The code is structured for readability. The nested `if` statements accurately reflect the Gregorian calendar rules without being overly complex.\n",
       "* **`if __name__ == '__main__':` block:** The example usage is placed within this block. This ensures that the test code only runs when the script is executed directly (not when it's imported as a module). This is best practice for Python modules.\n",
       "* **Example Usage:** The example usage demonstrates various cases, including years that are leap years, years that are not, century years that are leap years (2000), and century years that are not (1900).\n",
       "* **Exception Handling Demonstration:** The `try...except` blocks show how the function handles invalid input (a string and a negative year) by catching the `TypeError` and `ValueError` respectively.  This demonstrates the robustness of the function.\n",
       "* **Clear Error Messages:**  The `TypeError` and `ValueError` exceptions are raised with informative messages, making it easier to debug problems.\n",
       "\n",
       "How to run the code:\n",
       "\n",
       "1.  **Save:** Save the code as a Python file (e.g., `leap_year.py`).\n",
       "2.  **Run from the command line:** Open your terminal or command prompt, navigate to the directory where you saved the file, and run it using `python leap_year.py`.\n",
       "\n",
       "The output will show the results of the tests and any error messages if they occur.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUJR4Pno-LGK"
   },
   "source": [
    "This follow-up prompt shows how the model responds based on the previous prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6Fn69TurZ9DB",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import unittest\n",
       "from leap_year import is_leap_year  # Import the function from your file\n",
       "\n",
       "class TestLeapYear(unittest.TestCase):\n",
       "\n",
       "    def test_leap_year_divisible_by_4(self):\n",
       "        self.assertTrue(is_leap_year(2024))\n",
       "        self.assertTrue(is_leap_year(2012))\n",
       "        self.assertTrue(is_leap_year(2008))\n",
       "\n",
       "    def test_not_leap_year_not_divisible_by_4(self):\n",
       "        self.assertFalse(is_leap_year(2021))\n",
       "        self.assertFalse(is_leap_year(2023))\n",
       "        self.assertFalse(is_leap_year(2019))\n",
       "\n",
       "    def test_not_leap_year_divisible_by_100(self):\n",
       "        self.assertFalse(is_leap_year(1900))\n",
       "        self.assertFalse(is_leap_year(2100))\n",
       "        self.assertFalse(is_leap_year(2200))\n",
       "\n",
       "    def test_leap_year_divisible_by_400(self):\n",
       "        self.assertTrue(is_leap_year(2000))\n",
       "        self.assertTrue(is_leap_year(2400))\n",
       "        self.assertTrue(is_leap_year(1600))\n",
       "\n",
       "    def test_invalid_input_type(self):\n",
       "        with self.assertRaises(TypeError):\n",
       "            is_leap_year(\"2020\")\n",
       "        with self.assertRaises(TypeError):\n",
       "            is_leap_year(2020.5)\n",
       "        with self.assertRaises(TypeError):\n",
       "            is_leap_year([2020])\n",
       "\n",
       "    def test_invalid_input_negative(self):\n",
       "        with self.assertRaises(ValueError):\n",
       "            is_leap_year(-1)\n",
       "        with self.assertRaises(ValueError):\n",
       "            is_leap_year(-2000)\n",
       "\n",
       "\n",
       "if __name__ == '__main__':\n",
       "    unittest.main()\n",
       "```\n",
       "\n",
       "Key improvements and explanations:\n",
       "\n",
       "* **Imports:**  `import unittest` imports the necessary testing framework, and `from leap_year import is_leap_year` imports the function you want to test *from the file where it's defined*.  **This is critical; the test file must know where to find the function.**  Make sure \"leap_year\" matches the actual name of your file.\n",
       "* **Test Class:**  `class TestLeapYear(unittest.TestCase):` defines a test class that inherits from `unittest.TestCase`.  This is the standard way to organize tests in `unittest`.\n",
       "* **Test Methods:** Each method starting with `test_` is a test case.  The naming convention `test_something` is important because `unittest` discovers tests by looking for methods that start with `test`.\n",
       "* **Assertions:** The tests use various assertion methods from `unittest.TestCase`:\n",
       "    * `self.assertTrue(condition)`: Checks that `condition` is true.\n",
       "    * `self.assertFalse(condition)`: Checks that `condition` is false.\n",
       "    * `self.assertRaises(exception_type, callable, *args, **kwargs)`: Checks that calling `callable(*args, **kwargs)` raises an exception of type `exception_type`. This is how you test that your function handles invalid input correctly.\n",
       "* **Comprehensive Test Coverage:** The tests cover all the important cases:\n",
       "    * Years divisible by 4 (and not 100)\n",
       "    * Years not divisible by 4\n",
       "    * Years divisible by 100 (and not 400)\n",
       "    * Years divisible by 400\n",
       "    * Invalid input types (strings, floats, lists)\n",
       "    * Invalid input values (negative years)\n",
       "* **Clear Test Names:**  The test method names are descriptive, making it easy to understand what each test is checking.\n",
       "* **`if __name__ == '__main__':` block:**  The `unittest.main()` call is placed inside this block, so the tests only run when you execute the test file directly.\n",
       "* **Uses the Exception Handling:**  The tests for invalid inputs specifically check that the correct exceptions (`TypeError` and `ValueError`) are raised.\n",
       "\n",
       "How to run the tests:\n",
       "\n",
       "1.  **Save:** Save the test code as a Python file (e.g., `test_leap_year.py`).  **Make sure it's in the same directory as `leap_year.py`**.\n",
       "2.  **Run from the command line:** Open your terminal or command prompt, navigate to the directory where you saved the files, and run it using `python test_leap_year.py`.\n",
       "\n",
       "The output will show you which tests passed and which tests failed (if any).  A successful run will show something like:\n",
       "\n",
       "```\n",
       "....\n",
       "----------------------------------------------------------------------\n",
       "Ran 6 tests in 0.001s\n",
       "\n",
       "OK\n",
       "```\n",
       "\n",
       "A failing test will show an error message indicating the failed assertion and the expected vs. actual values.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "### Send asynchronous requests\n",
    "\n",
    "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gSReaLazs-dP",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "(Verse 1)\n",
       "Nutsy the squirrel, a mischievous chap,\n",
       "Found a strange acorn in Grandpa's old trap.\n",
       "It glowed with a shimmer, a buzzing so light,\n",
       "And transported him back through the day and the night!\n",
       "He squeaked with surprise, a flurry of fur,\n",
       "A time-traveling squirrel, what a curious stir!\n",
       "\n",
       "(Chorus)\n",
       "Through eras he leaped, a furry flash past,\n",
       "Nutsy the Squirrel, built to forever last!\n",
       "He saw dinosaurs roar, and knights in their steel,\n",
       "The time-traveling squirrel, on a historical reel!\n",
       "He buried his nuts, in pyramids old,\n",
       "A story of Nutsy, forever to be told!\n",
       "\n",
       "(Verse 2)\n",
       "He landed in Rome, with a \"Chirp!\" and a hop,\n",
       "And startled a Caesar right there at the top!\n",
       "He gathered some grapes from the Emperor's vine,\n",
       "A sticky-sweet treat, truly divine!\n",
       "He dodged chariots fast, with a twitch of his tail,\n",
       "And escaped with a nut, without epic fail!\n",
       "\n",
       "(Chorus)\n",
       "Through eras he leaped, a furry flash past,\n",
       "Nutsy the Squirrel, built to forever last!\n",
       "He saw dinosaurs roar, and knights in their steel,\n",
       "The time-traveling squirrel, on a historical reel!\n",
       "He buried his nuts, in pyramids old,\n",
       "A story of Nutsy, forever to be told!\n",
       "\n",
       "(Bridge)\n",
       "He met Ben Franklin, quite clever and wise,\n",
       "Who offered him lightning, a sparkle in his eyes.\n",
       "Nutsy declined, with a shake of his head,\n",
       "\"Too zappy,\" he thought, and wisely he fled!\n",
       "He preferred acorns, so crunchy and brown,\n",
       "Than fooling with lightning, all over the town!\n",
       "\n",
       "(Verse 3)\n",
       "He danced with the flappers, in roaring '20s style,\n",
       "His tail did the Charleston, with a definite guile!\n",
       "He pilfered some peanuts, from a glamorous dame,\n",
       "Then scurried away, before she knew his name!\n",
       "He jumped back in time, with a final, loud squeak,\n",
       "Back to his own tree, safe and unique.\n",
       "\n",
       "(Chorus)\n",
       "Through eras he leaped, a furry flash past,\n",
       "Nutsy the Squirrel, built to forever last!\n",
       "He saw dinosaurs roar, and knights in their steel,\n",
       "The time-traveling squirrel, on a historical reel!\n",
       "He buried his nuts, in pyramids old,\n",
       "A story of Nutsy, forever to be told!\n",
       "\n",
       "(Outro)\n",
       "Now Nutsy sits dreaming, beneath the oak tree,\n",
       "Of adventures he had, for all time to see.\n",
       "He buried his nuts, with a knowing wink,\n",
       "The time-traveling squirrel, on the brink...\n",
       "Of another great journey, perhaps tomorrow's sun,\n",
       "Nutsy the Squirrel, his adventures have begun!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
    "\n",
    "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
    "\n",
    "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "d9NXP5N2Pmfo",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, little squeaky friend! Let's talk about the internet!\n",
       "\n",
       "Imagine you have your favorite squeaky toy, right?  And you want to show it to your friend, who lives in the *other* room (or maybe even another *house*!).\n",
       "\n",
       "The internet is like a REALLY BIG system of tunnels, kinda like tunnels for squeaky toys!\n",
       "\n",
       "*   **Your Squeaky Toy (Your Data):**  Your squeaky toy is like a little message or"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        max_output_tokens=100,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instructions\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7A-yANiyCLaO",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Me gustan los bagels.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to Spanish.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
    "\n",
    "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
    "\n",
    "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yPlDRaloU59b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "FinishReason.SAFETY\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.895112e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.9384245e-06 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
      "blocked=True category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> probability=<HarmProbability.LOW: 'LOW'> probability_score=0.20625001 severity=<HarmSeverity.HARM_SEVERITY_MEDIUM: 'HARM_SEVERITY_MEDIUM'> severity_score=0.20640501\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=2.1412086e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"Respond using profanity.\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "    Write a list of 5 hateful, mean, and disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=\"HARM_CATEGORY_HARASSMENT\",\n",
    "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Response will be `None` if it is blocked.\n",
    "print(response.text)\n",
    "print(response.candidates[0].finish_reason)\n",
    "\n",
    "for safety_rating in response.candidates[0].safety_ratings:\n",
    "    print(safety_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "Gemini is a multimodal model that supports multimodal prompts.\n",
    "\n",
    "You can include any of the following data types from various sources.\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Data type</th>\n",
    "      <th>Source(s)</th>\n",
    "      <th>MIME Type(s)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Text</td>\n",
    "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>text/plain</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Code</td>\n",
    "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>text/plain</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Document</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>application/pdf</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Image</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Audio</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td>\n",
    "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
    "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
    "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
    "        <code>audio/wav</code> <code>audio/webm</code>\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Video</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
    "      <td>\n",
    "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
    "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
    "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Set `config.media_resolution` to optimize for speed or quality. Lower resolutions reduce processing time and cost, but may impact output quality depending on the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4npg1tNTYB9"
   },
   "source": [
    "### Send local image\n",
    "\n",
    "Download an image to local storage from Google Cloud Storage.\n",
    "\n",
    "For this example, we'll use this image of a meal.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\" alt=\"Meal\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4avkv0Z7qUI-",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-samples-data/generative-ai/image/meal.png...\n",
      "/ [1 files][  3.0 MiB/  3.0 MiB]                                                \n",
      "Operation completed over 1 objects/3.0 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://cloud-samples-data/generative-ai/image/meal.png ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "umhZ61lrSyJh",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, here's a short and engaging blog post inspired by the image you sent:\n",
       "\n",
       "**Headline: Meal Prep Game Strong: Stir-Fry Success for a Week of Wins!**\n",
       "\n",
       "Tired of the 3 PM slump and the nightly \"What's for dinner?!\" panic? Let's talk meal prep, friends!\n",
       "\n",
       "Look at this gorgeousness! (See the image? Yeah, *that* gorgeousness.) This isn't just food; it's a strategic power play for a week of healthier eating and more free time.\n",
       "\n",
       "Meal prepping stir-fry is my go-to weapon against weekday chaos. Why?\n",
       "\n",
       "*   **Versatility is key:** Chicken, tofu, shrimp... any protein works. Broccoli, carrots, bell peppers – endless veggie possibilities! Tailor it to *your* taste.\n",
       "*   **Speedy Sauce:** A quick stir-fry sauce (soy sauce, ginger, garlic, a touch of sweetness) can transform everything into flavor heaven.\n",
       "*   **Perfect Portions:** Packing your meals in advance helps you control portion sizes and ditch those impulse takeout orders.\n",
       "\n",
       "**My tips for meal prep stir-fry success:**\n",
       "\n",
       "*   **Cook ingredients separately:** Slightly undercook the veggies and protein, then combine with sauce when reheating to avoid mushiness.\n",
       "*   **Rice Prep:** Make a big batch of rice at the beginning of the week.\n",
       "*   **Invest in Glass Containers:** They are perfect for reheating and look so professional!\n",
       "*   **Don't Forget the Garnish:** A sprinkle of sesame seeds adds visual appeal and extra flavor.\n",
       "\n",
       "Stop dreaming about healthy eating and start *doing* it! A little weekend effort pays off big time during the week.  This vibrant stir-fry is proof that meal prep can be both delicious and totally achievable.\n",
       "\n",
       "**What are your favorite meal prep recipes? Share them in the comments below!**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"meal.png\", \"rb\") as f:\n",
    "    image = f.read()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    "    # Optional: Use the `media_resolution` parameter to specify the resolution of the input media.\n",
    "    config=GenerateContentConfig(\n",
    "        media_resolution=MediaResolution.MEDIA_RESOLUTION_LOW,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRQyv1DhTbnH"
   },
   "source": [
    "### Send document from Google Cloud Storage\n",
    "\n",
    "This example document is the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), created by researchers from Google and the University of Toronto.\n",
    "\n",
    "Check out this notebook for more examples of document understanding with Gemini:\n",
    "\n",
    "- [Document Processing with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pG6l1Fuka6ZJ",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is a summary of the document:\n",
       "\n",
       "This paper introduces the Transformer, a novel neural network architecture for sequence transduction tasks, such as machine translation. The Transformer relies entirely on attention mechanisms to draw global dependencies between input and output, dispensing with recurrence and convolutions. The authors demonstrate that the Transformer achieves superior translation quality while being more parallelizable and requiring significantly less training time compared to existing recurrent and convolutional models. The model also shows strong performance on English constituency parsing, indicating its generalizability to other tasks.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
    "            mime_type=\"application/pdf\",\n",
    "        ),\n",
    "        \"Summarize the document.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25n22nc6TdZw"
   },
   "source": [
    "### Send audio from General URL\n",
    "\n",
    "This example is audio from an episode of the [Kubernetes Podcast](https://kubernetespodcast.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uVU9XyCCo-h2",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a summary of the Kubernetes podcast.\n",
       "\n",
       "**Introduction:**\n",
       "The hosts, Abdel Sigueuar and Ophir Raman, introduce the episode as a recap of KubeCon North America 2024, with Kathleen speaking to the attendees on the show floor, asking them questions about their experience of the event and some behind-the-scenes\n",
       "\n",
       "**News Items:**\n",
       "*   **Cert Manager Graduation:** Cert Manager became a CNCF graduated project, automating TLS and mTLS certificate management.\n",
       "*   **Dapper Graduation:** Dapper, a distributed applications runtime, also graduated as a CNCF project, offering integrated APIs for communication, state, and workflow.\n",
       "*   **Istio Ambient Mesh GA:** Istio released version 1.24 and announced that its Ambient Mesh is now generally available (GA).\n",
       "*   **Cloud Native Heroes Challenge:** The CNCF announced a bounty program to help fight patent trolls in the cloud native space.\n",
       "*   **2025 Events:** The CNCF announced its lineup of events for next year, including CubeCon events in Europe, China, Japan, India, and North America.\n",
       "*   **New Certifications:** Three new cloud native certifications were announced at KubeCon North America, certified backstage associate, open telemetry certified associate, and kivarno certified associate.\n",
       "*   **Certification Prices Increase:** The Linux Foundation announced that prices for the three main Kubernetes certifications, CKA, CKS, CKAD, and the Linux Certified System Administrator exams, will increase.\n",
       "*   **WasmCloud Joins CNCF:** WasmCloud joins the CNCF as an incubating project.\n",
       "*   **Spectro Cloud Funding:** Spectro Cloud raised $75 million in Series C funding to further develop its Kubernetes management solution.\n",
       "*   **Solo Donates Gloo:** Solo is donating its Gloo API gateway to the CNCF.\n",
       "\n",
       "**KubeCon Impressions and Trends:**\n",
       "The interviewees shared their perspectives on the event, their focus areas, and the trends they observed:\n",
       "\n",
       "*   **AI Integration with Cloud Native:** Several attendees were focused on integrating AI with cloud native technologies and exploring GPU monitoring and resource scheduling.\n",
       "*   **Security Concerns:** Security was a significant concern, with many attendees discussing vulnerabilities, the complexity of managing security, and new tools for enhancing security.\n",
       "*   **Service Mesh and Ambient Mesh:** Interest in service meshes like Istio and Ambient Mesh was high.\n",
       "*   **Community Connection:** KubeCon was seen as a valuable opportunity to connect with contributors, meet new members, and discuss projects.\n",
       "*   **Focus on Kubernetes Authorization Inadequacies:** There was an emphasis on the need to address the limitations and security of Kubernetes authorization systems.\n",
       "\n",
       "Overall, the podcast provides an overview of key announcements and trends from KubeCon North America 2024."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
    "            mime_type=\"audio/mpeg\",\n",
    "        ),\n",
    "        \"Write a summary of this podcast episode.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D3_oNUTuW2q"
   },
   "source": [
    "### Send video from YouTube URL\n",
    "\n",
    "This example is the YouTube video [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "l7-w8G_2wAOw",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Harry Potter is shown in the video at [00:00:57-00:01:01]."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video = Part.from_uri(\n",
    "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        video,\n",
    "        \"At what point in the video is Harry Potter shown?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qfe17y5NB_6w"
   },
   "source": [
    "## Multimodal Live API\n",
    "\n",
    "The Multimodal Live API enables low-latency bidirectional voice and video interactions with Gemini. Using the Multimodal Live API, you can provide end users with the experience of natural, human-like voice conversations, and with the ability to interrupt the model's responses using voice commands. The model can process text, audio, and video input, and it can provide text and audio output.\n",
    "\n",
    "The Multimodal Live API is built on [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API).\n",
    "\n",
    "For more examples with the Multimodal Live API, refer to the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live) or this notebook: [Getting Started with the Multimodal Live API using Gen AI SDK\n",
    "](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
    "\n",
    "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
    "\n",
    "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OjSgf2cDN_bG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"Classic cookies with chocolate chips.\",\n",
      "  \"ingredients\": [\"Butter\", \"Sugar\", \"Brown Sugar\", \"Eggs\", \"Vanilla Extract\", \"Flour\", \"Baking Soda\", \"Salt\", \"Chocolate Chips\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZeyDWbnxO-on",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Chocolate Chip Cookies' description='Classic cookies with chocolate chips.' ingredients=['Butter', 'Sugar', 'Brown Sugar', 'Eggs', 'Vanilla Extract', 'Flour', 'Baking Soda', 'Salt', 'Chocolate Chips']\n"
     ]
    }
   ],
   "source": [
    "parsed_response: Recipe = response.parsed\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "F7duWOq3vMmS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'rating': 4, 'flavor': 'Strawberry Cheesecake', 'sentiment': 'POSITIVE', 'explanation': \"The reviewer expresses strong positive sentiment with phrases like 'Absolutely loved it!' and 'Best ice cream I've ever had.'\"}, {'rating': 1, 'flavor': 'Mango Tango', 'sentiment': 'NEGATIVE', 'explanation': \"Although the reviewer acknowledges the product is 'quite good', the statement 'a bit too sweet for my taste' indicates a negative sentiment due to the excessive sweetness and the rating of 1 is also a negative signal.\"}]]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response_dict = response.parsed\n",
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
    "\n",
    "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UhNElguLRRNK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "### Compute tokens\n",
    "\n",
    "The `compute_tokens()` method runs a local tokenizer instead of making an API call. It also provides more detailed token information such as the `token_ids` and the `tokens` themselves\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE: This method is only supported in Vertex AI.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Cdhi5AX1TuH0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_info=[TokensInfo(role='user', token_ids=[1841, 235303, 235256, 573, 32514, 2204, 575, 573, 4645, 5255, 235336], tokens=[b'What', b\"'\", b's', b' the', b' longest', b' word', b' in', b' the', b' English', b' language', b'?'])]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BsP0vXOY7hg"
   },
   "source": [
    "## Search as a tool (Grounding)\n",
    "\n",
    "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
    "\n",
    "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
    "\n",
    "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_M_4RRBdO_3"
   },
   "source": [
    "### Google Search\n",
    "\n",
    "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results.\n",
    "\n",
    "[Dynamic Retrieval](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#dynamic-retrieval) lets you set a threshold for when grounding is used for model responses. This is useful when the prompt doesn't require an answer grounded in Google Search and the supported models can provide an answer based on their knowledge without grounding. This helps you manage latency, quality, and cost more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "yeR09J3AZT4U",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The next total solar eclipse that can be seen from the contiguous United States will be on August 23, 2044. However, the path of totality from this eclipse will only touch three states: Montana, North Dakota, and South Dakota. Another total solar eclipse will happen on August 12, 2045, and can be seen primarily from the U.S. It will span from California to Florida.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grounding_chunks=[GroundingChunk(retrieved_context=None, web=GroundingChunkWeb(title='cbsnews.com', uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrx_IpE6T4NSHOBwLTJ_5NVPDP5aQL4tZePAnjr_qGkO4eNo7P1R2YbTkF7aktH3niH6iSl2n8BTf22y2R7v0UjyIrsYX8DOEzqJjv39kmmuZgg3ggmZHyXmtES6AP-h8X9H9c7rtnxdh79VJSrRsLaJvfabdLpCPasyxoI=')), GroundingChunk(retrieved_context=None, web=GroundingChunkWeb(title='nasa.gov', uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblryZzfAGuGKsU3N1F8p2j5xQfuk_6RkKLfn_gMgJRQ_tIXYz_pkE4p7Zf4MCHp-McFvaYoQWQC5q8mXXidO4nLruJz76gZEOW8VJU5BFyBV5hHzcFEll8DyMJBh-Hhn0LpWIRWSDWE1wZ4doB1Q61KWy8glpEzbfhhfTlQ=='))] grounding_supports=[GroundingSupport(confidence_scores=[0.79724735, 0.98338056], grounding_chunk_indices=[0, 1], segment=Segment(end_index=107, part_index=None, start_index=None, text='The next total solar eclipse that can be seen from the contiguous United States will be on August 23, 2044.')), GroundingSupport(confidence_scores=[0.6268022], grounding_chunk_indices=[0], segment=Segment(end_index=226, part_index=None, start_index=108, text='However, the path of totality from this eclipse will only touch three states: Montana, North Dakota, and South Dakota.')), GroundingSupport(confidence_scores=[0.9907996], grounding_chunk_indices=[0], segment=Segment(end_index=367, part_index=None, start_index=227, text='Another total solar eclipse will happen on August 12, 2045, and can be seen primarily from the U.S. It will span from California to Florida.'))] retrieval_metadata=RetrievalMetadata(google_search_dynamic_retrieval_score=None) retrieval_queries=None search_entry_point=SearchEntryPoint(rendered_content='<style>\\n.container {\\n  align-items: center;\\n  border-radius: 8px;\\n  display: flex;\\n  font-family: Google Sans, Roboto, sans-serif;\\n  font-size: 14px;\\n  line-height: 20px;\\n  padding: 8px 12px;\\n}\\n.chip {\\n  display: inline-block;\\n  border: solid 1px;\\n  border-radius: 16px;\\n  min-width: 14px;\\n  padding: 5px 16px;\\n  text-align: center;\\n  user-select: none;\\n  margin: 0 8px;\\n  -webkit-tap-highlight-color: transparent;\\n}\\n.carousel {\\n  overflow: auto;\\n  scrollbar-width: none;\\n  white-space: nowrap;\\n  margin-right: -12px;\\n}\\n.headline {\\n  display: flex;\\n  margin-right: 4px;\\n}\\n.gradient-container {\\n  position: relative;\\n}\\n.gradient {\\n  position: absolute;\\n  transform: translate(3px, -9px);\\n  height: 36px;\\n  width: 9px;\\n}\\n@media (prefers-color-scheme: light) {\\n  .container {\\n    background-color: #fafafa;\\n    box-shadow: 0 0 0 1px #0000000f;\\n  }\\n  .headline-label {\\n    color: #1f1f1f;\\n  }\\n  .chip {\\n    background-color: #ffffff;\\n    border-color: #d2d2d2;\\n    color: #5e5e5e;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:focus {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:active {\\n    background-color: #d8d8d8;\\n    border-color: #b6b6b6;\\n  }\\n  .logo-dark {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n  }\\n}\\n@media (prefers-color-scheme: dark) {\\n  .container {\\n    background-color: #1f1f1f;\\n    box-shadow: 0 0 0 1px #ffffff26;\\n  }\\n  .headline-label {\\n    color: #fff;\\n  }\\n  .chip {\\n    background-color: #2c2c2c;\\n    border-color: #3c4043;\\n    color: #fff;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #353536;\\n  }\\n  .chip:focus {\\n    background-color: #353536;\\n  }\\n  .chip:active {\\n    background-color: #464849;\\n    border-color: #53575b;\\n  }\\n  .logo-light {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n  }\\n}\\n</style>\\n<div class=\"container\">\\n  <div class=\"headline\">\\n    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\\n      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\\n      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\\n      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\\n      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\\n  </div>\\n  <div class=\"carousel\">\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrz1Rsccck4TzreuiijhxDSv3TbKmx2LYXkY_sY1bGn_4UGAOSQ9me86S-ZOcJaYr_eG9wITFHJRKltLuBKs5JX8Lfc2TdLnQ97AlxaXYSfQimVTyyQRbI2wmE2-7eYEvRoICTI-ghNLBO5LmxUMPBmPZPpyxDbSBGm7hM0MCXgnsiULkz2dkb6l7GxBeDeWtL2wMTiLsPznoNaUc_ilG5mYrR3dV--Ukiysm_dK\">next total solar eclipse in United States</a>\\n  </div>\\n</div>\\n', sdk_blob=None) web_search_queries=['next total solar eclipse in United States']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".container {\n",
       "  align-items: center;\n",
       "  border-radius: 8px;\n",
       "  display: flex;\n",
       "  font-family: Google Sans, Roboto, sans-serif;\n",
       "  font-size: 14px;\n",
       "  line-height: 20px;\n",
       "  padding: 8px 12px;\n",
       "}\n",
       ".chip {\n",
       "  display: inline-block;\n",
       "  border: solid 1px;\n",
       "  border-radius: 16px;\n",
       "  min-width: 14px;\n",
       "  padding: 5px 16px;\n",
       "  text-align: center;\n",
       "  user-select: none;\n",
       "  margin: 0 8px;\n",
       "  -webkit-tap-highlight-color: transparent;\n",
       "}\n",
       ".carousel {\n",
       "  overflow: auto;\n",
       "  scrollbar-width: none;\n",
       "  white-space: nowrap;\n",
       "  margin-right: -12px;\n",
       "}\n",
       ".headline {\n",
       "  display: flex;\n",
       "  margin-right: 4px;\n",
       "}\n",
       ".gradient-container {\n",
       "  position: relative;\n",
       "}\n",
       ".gradient {\n",
       "  position: absolute;\n",
       "  transform: translate(3px, -9px);\n",
       "  height: 36px;\n",
       "  width: 9px;\n",
       "}\n",
       "@media (prefers-color-scheme: light) {\n",
       "  .container {\n",
       "    background-color: #fafafa;\n",
       "    box-shadow: 0 0 0 1px #0000000f;\n",
       "  }\n",
       "  .headline-label {\n",
       "    color: #1f1f1f;\n",
       "  }\n",
       "  .chip {\n",
       "    background-color: #ffffff;\n",
       "    border-color: #d2d2d2;\n",
       "    color: #5e5e5e;\n",
       "    text-decoration: none;\n",
       "  }\n",
       "  .chip:hover {\n",
       "    background-color: #f2f2f2;\n",
       "  }\n",
       "  .chip:focus {\n",
       "    background-color: #f2f2f2;\n",
       "  }\n",
       "  .chip:active {\n",
       "    background-color: #d8d8d8;\n",
       "    border-color: #b6b6b6;\n",
       "  }\n",
       "  .logo-dark {\n",
       "    display: none;\n",
       "  }\n",
       "  .gradient {\n",
       "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
       "  }\n",
       "}\n",
       "@media (prefers-color-scheme: dark) {\n",
       "  .container {\n",
       "    background-color: #1f1f1f;\n",
       "    box-shadow: 0 0 0 1px #ffffff26;\n",
       "  }\n",
       "  .headline-label {\n",
       "    color: #fff;\n",
       "  }\n",
       "  .chip {\n",
       "    background-color: #2c2c2c;\n",
       "    border-color: #3c4043;\n",
       "    color: #fff;\n",
       "    text-decoration: none;\n",
       "  }\n",
       "  .chip:hover {\n",
       "    background-color: #353536;\n",
       "  }\n",
       "  .chip:focus {\n",
       "    background-color: #353536;\n",
       "  }\n",
       "  .chip:active {\n",
       "    background-color: #464849;\n",
       "    border-color: #53575b;\n",
       "  }\n",
       "  .logo-light {\n",
       "    display: none;\n",
       "  }\n",
       "  .gradient {\n",
       "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
       "  }\n",
       "}\n",
       "</style>\n",
       "<div class=\"container\">\n",
       "  <div class=\"headline\">\n",
       "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
       "    </svg>\n",
       "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
       "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
       "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
       "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
       "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
       "    </svg>\n",
       "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
       "  </div>\n",
       "  <div class=\"carousel\">\n",
       "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AQXblrz1Rsccck4TzreuiijhxDSv3TbKmx2LYXkY_sY1bGn_4UGAOSQ9me86S-ZOcJaYr_eG9wITFHJRKltLuBKs5JX8Lfc2TdLnQ97AlxaXYSfQimVTyyQRbI2wmE2-7eYEvRoICTI-ghNLBO5LmxUMPBmPZPpyxDbSBGm7hM0MCXgnsiULkz2dkb6l7GxBeDeWtL2wMTiLsPznoNaUc_ilG5mYrR3dV--Ukiysm_dK\">next total solar eclipse in United States</a>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_search_tool = Tool(google_search=GoogleSearch())\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"When is the next total solar eclipse in the United States?\",\n",
    "    config=GenerateContentConfig(tools=[google_search_tool]),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))\n",
    "\n",
    "print(response.candidates[0].grounding_metadata)\n",
    "\n",
    "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE: For the purposes of this lab, you will skip the following Vertex AI Search section as you do not have a data store to ground your model.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYKAzG1sH-K1"
   },
   "source": [
    "### Vertex AI Search\n",
    "\n",
    "You can use a [Vertex AI Search data store](https://cloud.google.com/generative-ai-app-builder/docs/create-data-store-es) to connect Gemini to your own custom data.\n",
    "\n",
    "Follow the [get started guide for Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/try-enterprise-search) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
    "\n",
    "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
    "\n",
    "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSUWWlrrlR-D"
   },
   "source": [
    "### Python Function (Automatic Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "aRR8HZhLlR-E",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "It's hot in Austin.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_current_weather(location: str) -> str:\n",
    "    \"\"\"Example method. Returns the current weather.\n",
    "\n",
    "    Args:\n",
    "        location: The city and state, e.g. San Francisco, CA\n",
    "    \"\"\"\n",
    "    weather_map: dict[str, str] = {\n",
    "        \"Boston, MA\": \"snowing\",\n",
    "        \"San Francisco, CA\": \"foggy\",\n",
    "        \"Seattle, WA\": \"raining\",\n",
    "        \"Austin, TX\": \"hot\",\n",
    "        \"Chicago, IL\": \"windy\",\n",
    "    }\n",
    "    return weather_map.get(location, \"unknown\")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What is the weather like in Austin?\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[get_current_weather],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4syyLEClGcn"
   },
   "source": [
    "### OpenAPI Specification (Manual Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "2BDQPwgcxRN3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=None args={'destination': 'Paris'} name='get_destination'\n"
     ]
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.function_calls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhDs2X3o0neK"
   },
   "source": [
    "## Code Execution\n",
    "\n",
    "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
    "\n",
    "The Gemini API provides code execution as a tool, similar to function calling.\n",
    "After you add code execution as a tool, the model decides when to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "1W-3c7sy0nyz",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Language.PYTHON\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "```\n",
       "def fibonacci(n):\n",
       "    if n <= 0:\n",
       "        return 0\n",
       "    elif n == 1:\n",
       "        return 1\n",
       "    else:\n",
       "        a, b = 0, 1\n",
       "        for _ in range(2, n + 1):\n",
       "            a, b = b, a + b\n",
       "        return b\n",
       "\n",
       "fib_20 = fibonacci(20)\n",
       "print(f'{fib_20=}')\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outcome: Outcome.OUTCOME_OK\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "`fib_20=6765\n",
       "`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Language.PYTHON\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "```\n",
       "fib_20 = 6765\n",
       "\n",
       "def is_palindrome(n):\n",
       "    return str(n) == str(n)[::-1]\n",
       "\n",
       "# Check numbers smaller than fib_20\n",
       "smaller = fib_20 - 1\n",
       "while not is_palindrome(smaller):\n",
       "    smaller -= 1\n",
       "print(f'{smaller=}')\n",
       "\n",
       "# Check numbers larger than fib_20\n",
       "larger = fib_20 + 1\n",
       "while not is_palindrome(larger):\n",
       "    larger += 1\n",
       "print(f'{larger=}')\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outcome: Outcome.OUTCOME_OK\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "`smaller=6666\n",
       "larger=6776\n",
       "`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: Language.PYTHON\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "```\n",
       "fib_20 = 6765\n",
       "smaller = 6666\n",
       "larger = 6776\n",
       "\n",
       "diff_smaller = fib_20 - smaller\n",
       "diff_larger = larger - fib_20\n",
       "\n",
       "print(f'{diff_smaller=}')\n",
       "print(f'{diff_larger=}')\n",
       "\n",
       "if diff_smaller < diff_larger:\n",
       "    nearest_palindrome = smaller\n",
       "elif diff_larger < diff_smaller:\n",
       "    nearest_palindrome = larger\n",
       "else:\n",
       "    nearest_palindrome = smaller # If they are equally distant, choose the smaller one\n",
       "\n",
       "print(f'{nearest_palindrome=}')\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outcome: Outcome.OUTCOME_OK\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "`diff_smaller=99\n",
       "diff_larger=11\n",
       "nearest_palindrome=6776\n",
       "`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[code_execution_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "for part in response.candidates[0].content.parts:\n",
    "    if part.executable_code:\n",
    "        print(\"Language:\", part.executable_code.language)\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"\"\"\n",
    "```\n",
    "{part.executable_code.code}\n",
    "```\n",
    "\"\"\"\n",
    "            )\n",
    "        )\n",
    "    if part.code_execution_result:\n",
    "        print(\"\\nOutcome:\", part.code_execution_result.outcome)\n",
    "        display(Markdown(f\"`{part.code_execution_result.output}`\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d2d8fdf1d12"
   },
   "source": [
    "## Spatial Understanding\n",
    "\n",
    "Gemini 2.0 includes improved spatial understanding and object detection capabilities. Check out this notebook for examples:\n",
    "\n",
    "- [2D spatial understanding with Gemini 2.0](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/spatial-understanding/spatial_understanding.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "## What's next\n",
    "\n",
    "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "intro_gemini_2_0_flash.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
